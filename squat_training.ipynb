{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Squat quality scoring with TensorFlow\n",
    "\n",
    "This notebook trains a small BlazePose + LSTM regressor on your local squat videos and predicts a quality score per clip.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-notes",
   "metadata": {},
   "source": [
    "## 1) Environment setup\n",
    "- Installs TensorFlow + MediaPipe + OpenCV.\n",
    "- Use your own data/squats_train and data/squats_test folders; no downloads required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!python -m pip install \"tensorflow<2.17\" tensorflow-io jupyter\n",
    "!python -m pip install mediapipe opencv-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-config",
   "metadata": {},
   "source": [
    "## 2) Imports and configuration\n",
    "- Adjust paths or scoring scale if needed.\n",
    "- Scores are expected in data/squat_scores.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n",
      "Data root: C:\\Users\\KarthikPC\\vscode_projects\\CS663_Project2_training\\data\n",
      "Model dir: C:\\Users\\KarthikPC\\vscode_projects\\CS663_Project2_training\\checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Seed for deterministic behavior where possible (shuffling, TF ops)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Training / preprocessing hyper-parameters and constants\n",
    "BATCH_SIZE = 16\n",
    "NUM_FRAMES = 16            # number of frames sampled per video (temporal length)\n",
    "IMG_SIZE = 160             # image-ops size used in earlier versions (not used by pose extractor)\n",
    "NUM_LANDMARKS = 33         # BlazePose outputs 33 landmarks per frame\n",
    "LANDMARK_DIMS = 4          # per-landmark dims: (x, y, z, visibility)\n",
    "SCORE_SCALE = 100.0        # label scale for human readable scores (labels stored 0..100, model trains on 0..1)\n",
    "\n",
    "# Paths used throughout the notebook\n",
    "DATA_ROOT = Path(\"data\")\n",
    "TRAIN_DIR = DATA_ROOT / \"squats_train\"\n",
    "TEST_DIR = DATA_ROOT / \"squats_test\"\n",
    "LABELS_PATH = DATA_ROOT / \"squat_scores.csv\"\n",
    "MODEL_DIR = Path(\"checkpoints\")\n",
    "\n",
    "# Allowed video extensions when scanning directories\n",
    "VIDEO_EXTS = (\".mp4\", \".mov\", \".avi\", \".mkv\")\n",
    "\n",
    "# Ensure disk layout exists before proceeding\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helpful prints for users running the notebook interactively\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('Data root:', DATA_ROOT.resolve())\n",
    "print('Model dir:', MODEL_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download",
   "metadata": {},
   "source": [
    "## 3) Prepare local data and labels\n",
    "- A CSV template is generated listing every video under data/squats_train and data/squats_test.\n",
    "- Fill in the score column (0-100). At least two labeled train videos are required to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label file ready at data\\squat_scores.csv. Fill in 'score' (0-100) for each row.\n"
     ]
    }
   ],
   "source": [
    "def list_videos(root: Path):\n",
    "    return sorted(\n",
    "        p for p in root.rglob(\"*\")\n",
    "        if p.suffix.lower() in VIDEO_EXTS and p.is_file()\n",
    "    )\n",
    "\n",
    "\n",
    "def ensure_label_file():\n",
    "    existing = {}\n",
    "    if LABELS_PATH.exists():\n",
    "        with LABELS_PATH.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing[row.get(\"relative_path\", \"\")] = row.get(\"score\", \"\")\n",
    "\n",
    "    rows = []\n",
    "    for p in list_videos(TRAIN_DIR) + list_videos(TEST_DIR):\n",
    "        rel = p.relative_to(DATA_ROOT).as_posix()\n",
    "        rows.append({\"relative_path\": rel, \"score\": existing.get(rel, \"\")})\n",
    "\n",
    "    LABELS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with LABELS_PATH.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"relative_path\", \"score\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"Label file ready at {LABELS_PATH}. Fill in 'score' (0-{int(SCORE_SCALE)}) for each row.\")\n",
    "    return rows\n",
    "\n",
    "\n",
    "_ = ensure_label_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summaries",
   "metadata": {},
   "source": [
    "### Label summary / sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "summary-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 132 labeled samples.\n"
     ]
    }
   ],
   "source": [
    "def load_labeled_samples():\n",
    "    samples = []\n",
    "    missing = []\n",
    "    with LABELS_PATH.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            rel = row.get(\"relative_path\", \"\")\n",
    "            score_str = row.get(\"score\", \"\").strip()\n",
    "            if not rel:\n",
    "                continue\n",
    "\n",
    "            full = DATA_ROOT / rel\n",
    "            if not full.exists():\n",
    "                missing.append(rel)\n",
    "                continue\n",
    "\n",
    "            if not score_str:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                score = float(score_str)\n",
    "            except ValueError:\n",
    "                print(f\"Skipping {rel}: invalid score '{score_str}'\")\n",
    "                continue\n",
    "\n",
    "            score = max(0.0, min(SCORE_SCALE, score))\n",
    "            samples.append((str(full), score))\n",
    "\n",
    "    if missing:\n",
    "        print(\"Warning: paths not found on disk:\", missing)\n",
    "\n",
    "    print(f\"Loaded {len(samples)} labeled samples.\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "labeled_samples = load_labeled_samples()\n",
    "if len(labeled_samples) < 2:\n",
    "    raise ValueError(\"Add scores in the CSV (at least 2 labeled videos) before training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tfdata",
   "metadata": {},
   "source": [
    "## 4) Build a TensorFlow input pipeline\n",
    "- Uniformly sample frames, run BlazePose to get 33 landmarks per frame, normalize, and emit flattened keypoints.\n",
    "- Labels are normalized to 0-1 during training; final scores are rescaled to 0-100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 5\n",
      "Val batches: 2\n",
      "Test batches: 2\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "\n",
    "def _sample_frame_indices(total_frames: int, num_target: int) -> np.ndarray:\n",
    "    \"\"\"Return `num_target` frame indices sampled evenly across the video.\n",
    "\n",
    "    Notes:\n",
    "      - If total_frames is 0 => returns zeros (preserves dtype/shape expected by callers).\n",
    "      - Returned dtype is np.int32 to match indexing expectations in downstream code.\n",
    "    \"\"\"\n",
    "    if total_frames <= 0:\n",
    "        return np.zeros((num_target,), dtype=np.int32)\n",
    "    idxs = np.linspace(0, max(total_frames - 1, 0), num_target).astype(np.int32)\n",
    "    return idxs\n",
    "\n",
    "\n",
    "def _normalize_landmarks(landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Robust per-frame normalization & rotation for landmarks.\n",
    "\n",
    "    Input:\n",
    "      - landmarks: shape (NUM_LANDMARKS, LANDMARK_DIMS) with columns (x, y, z, visibility)\n",
    "    Output:\n",
    "      - normalized landmarks (same shape) where:\n",
    "          * coordinates are centered (hip midpoint or visible mean)\n",
    "          * scaled by torso/hip size to produce scale-invariant features\n",
    "          * rotated so the major body axis is horizontal (helps the model focus on pose shape)\n",
    "      - returns None if there are fewer than two sufficiently-visible points in the frame\n",
    "\n",
    "    The function uses visibility thresholds and layered fallbacks (hip midpoint -> shoulders -> PCA)\n",
    "    so the pipeline remains robust with occasional missing detections.\n",
    "    \"\"\"\n",
    "    # Use visibility flag (4th column) to decide which points are reliable\n",
    "    vis = landmarks[:, 3] >= 0.25\n",
    "    if vis.sum() < 2:\n",
    "        # Not enough points — prefer to skip this frame (caller can interpolate later)\n",
    "        return None\n",
    "\n",
    "    def valid_pair(i, j):\n",
    "        try:\n",
    "            return bool(vis[i] and vis[j])\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    LEFT_HIP, RIGHT_HIP = 23, 24\n",
    "    LEFT_SHOULDER, RIGHT_SHOULDER = 11, 12\n",
    "\n",
    "    # Choose a stable center point: hip midpoint if available, else mean of visible coords\n",
    "    if valid_pair(LEFT_HIP, RIGHT_HIP):\n",
    "        left_hip, right_hip = landmarks[LEFT_HIP, :3], landmarks[RIGHT_HIP, :3]\n",
    "        center_hip = (left_hip + right_hip) / 2.0\n",
    "    else:\n",
    "        visible_coords = landmarks[vis, :3]\n",
    "        center_hip = visible_coords.mean(axis=0)\n",
    "\n",
    "    # Choose a shoulder center if possible (used to compute scale/torso length)\n",
    "    if valid_pair(LEFT_SHOULDER, RIGHT_SHOULDER):\n",
    "        left_sh, right_sh = landmarks[LEFT_SHOULDER, :3], landmarks[RIGHT_SHOULDER, :3]\n",
    "        center_sh = (left_sh + right_sh) / 2.0\n",
    "    elif valid_pair(LEFT_HIP, RIGHT_HIP):\n",
    "        # crude fallback: estimate shoulder position above the hip center\n",
    "        center_sh = center_hip + np.array([0.0, 0.5, 0.0])\n",
    "    else:\n",
    "        center_sh = center_hip + np.array([0.0, 0.5, 0.0])\n",
    "\n",
    "    # scale estimate — torso length or hip separation (pixels normalized to frame size by BlazePose)\n",
    "    torso = np.linalg.norm(center_sh[:2] - center_hip[:2])\n",
    "    hip_dist = np.linalg.norm(landmarks[LEFT_HIP, :2] - landmarks[RIGHT_HIP, :2]) if valid_pair(LEFT_HIP, RIGHT_HIP) else torso\n",
    "    scale = max(torso, hip_dist, 1e-3)\n",
    "\n",
    "    # shift & scale (in-place) -> avoid mutating caller by copying earlier in pipeline\n",
    "    landmarks[:, :3] = (landmarks[:, :3] - center_hip) / scale\n",
    "\n",
    "    # compute rotation vector: prefer hips -> shoulders -> PCA fallback\n",
    "    if valid_pair(LEFT_HIP, RIGHT_HIP):\n",
    "        hip_vec = (landmarks[RIGHT_HIP, :2] - landmarks[LEFT_HIP, :2])\n",
    "    elif valid_pair(LEFT_SHOULDER, RIGHT_SHOULDER):\n",
    "        hip_vec = (landmarks[RIGHT_SHOULDER, :2] - landmarks[LEFT_SHOULDER, :2])\n",
    "    else:\n",
    "        visible_pts = landmarks[vis, :2]\n",
    "        if visible_pts.shape[0] < 2:\n",
    "            hip_vec = np.array([1.0, 0.0])\n",
    "        else:\n",
    "            pts_centered = visible_pts - visible_pts.mean(axis=0)\n",
    "            u, s, vh = np.linalg.svd(pts_centered, full_matrices=False)\n",
    "            hip_vec = vh[0]\n",
    "\n",
    "    # rotate so the main body axis is horizontal (small epsilon avoids division-by-zero)\n",
    "    angle = np.arctan2(hip_vec[1], hip_vec[0] + 1e-6)\n",
    "    cos_a, sin_a = np.cos(-angle), np.sin(-angle)\n",
    "    rot = np.array([[cos_a, -sin_a], [sin_a, cos_a]], dtype=np.float32)\n",
    "    landmarks[:, :2] = landmarks[:, :2] @ rot.T\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def _extract_keypoints_np(video_path: str) -> np.ndarray:\n",
    "    \"\"\"Extract normalized per-frame keypoint features for NUM_FRAMES timesteps.\n",
    "\n",
    "    Output is a (NUM_FRAMES, NUM_LANDMARKS * LANDMARK_DIMS + 1) array where the\n",
    "    last column is a per-frame validity flag (1.0 = original frame had pose, 0.0 = interpolated).\n",
    "\n",
    "    The function implements caching (sha1 of the path) to speed repeated runs on the\n",
    "    same video. Missing frames are temporally interpolated so the sequence is dense.\n",
    "    \"\"\"\n",
    "    import hashlib\n",
    "\n",
    "    cache_dir = MODEL_DIR / \"keypoints_cache\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    h = hashlib.sha1(video_path.encode(\"utf-8\")).hexdigest()\n",
    "    cache_file = cache_dir / f\"{h}.npy\"\n",
    "    if cache_file.exists():\n",
    "        try:\n",
    "            return np.load(str(cache_file))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    num_frames = len(frames)\n",
    "    keypoints = np.full((NUM_FRAMES, NUM_LANDMARKS, LANDMARK_DIMS), np.nan, dtype=np.float32)\n",
    "    if num_frames == 0:\n",
    "        # nothing to do — return zeros + invalid mask\n",
    "        filled = np.zeros((NUM_FRAMES, NUM_LANDMARKS * LANDMARK_DIMS), dtype=np.float32)\n",
    "        valid_mask = np.zeros((NUM_FRAMES,), dtype=np.float32)\n",
    "        out = np.concatenate([filled, valid_mask[:, None]], axis=1)\n",
    "        np.save(str(cache_file), out)\n",
    "        return out\n",
    "\n",
    "    idxs = _sample_frame_indices(num_frames, NUM_FRAMES)\n",
    "\n",
    "    mp_pose_local = mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        enable_segmentation=False,\n",
    "        smooth_landmarks=True,\n",
    "    )\n",
    "\n",
    "    valid_per_frame = np.zeros((NUM_FRAMES,), dtype=bool)\n",
    "    for out_i, frame_idx in enumerate(idxs):\n",
    "        frame = frames[int(frame_idx)]\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = mp_pose_local.process(image_rgb)\n",
    "        if results.pose_landmarks:\n",
    "            lm = results.pose_landmarks.landmark\n",
    "            coords = np.array([[p.x, p.y, p.z, p.visibility] for p in lm], dtype=np.float32)\n",
    "            norm = _normalize_landmarks(coords)\n",
    "            if norm is not None:\n",
    "                keypoints[out_i] = norm\n",
    "                valid_per_frame[out_i] = True\n",
    "\n",
    "    try:\n",
    "        mp_pose_local.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # interpolate missing values for each landmark/dimension across the temporal axis\n",
    "    idxs_time = np.arange(NUM_FRAMES)\n",
    "    for li in range(NUM_LANDMARKS):\n",
    "        for d in range(LANDMARK_DIMS):\n",
    "            series = keypoints[:, li, d]\n",
    "            good = ~np.isnan(series)\n",
    "            if good.any():\n",
    "                keypoints[:, li, d] = np.interp(idxs_time, idxs_time[good], series[good])\n",
    "            else:\n",
    "                keypoints[:, li, d] = 0.0\n",
    "\n",
    "    frame_valid = valid_per_frame.astype(np.float32)\n",
    "\n",
    "    # Flatten and append the per-frame validity flag as an extra feature column\n",
    "    keypoints_flat = keypoints.reshape((NUM_FRAMES, NUM_LANDMARKS * LANDMARK_DIMS))\n",
    "    out = np.concatenate([keypoints_flat, frame_valid[:, None]], axis=1).astype(np.float32)\n",
    "\n",
    "    try:\n",
    "        np.save(str(cache_file), out)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_keypoints(path: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Wrapper for tf.data pipeline.\n",
    "\n",
    "    - Accepts a tf.Tensor path and uses tf.py_function to call the Python preprocessing above.\n",
    "    - Returns a tf.Tensor with shape (NUM_FRAMES, NUM_LANDMARKS * LANDMARK_DIMS + 1) and dtype tf.float32.\n",
    "    \"\"\"\n",
    "    def _py_decode(p):\n",
    "        return _extract_keypoints_np(p.numpy().decode(\"utf-8\"))\n",
    "\n",
    "    kpts = tf.py_function(_py_decode, [path], tf.float32)\n",
    "    # shape = NUM_FRAMES x (NUM_LANDMARKS * LANDMARK_DIMS + 1)\n",
    "    kpts.set_shape((NUM_FRAMES, NUM_LANDMARKS * LANDMARK_DIMS + 1))\n",
    "    return kpts\n",
    "\n",
    "\n",
    "def preprocess(path: tf.Tensor, score: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Dataset mapping function.\n",
    "\n",
    "    - Loads the keypoints for `path` (video path), returns (features, label) pair.\n",
    "    - The label is normalized to 0..1 (model training range) and reshaped to (1,) per sample.\n",
    "    \"\"\"\n",
    "    keypoints = load_keypoints(path)\n",
    "    # already (NUM_FRAMES, features); no reshape needed\n",
    "    score = tf.cast(score, tf.float32) / SCORE_SCALE\n",
    "    score = tf.expand_dims(score, axis=-1)\n",
    "    return keypoints, score\n",
    "\n",
    "\n",
    "def build_tf_dataset(samples, training: bool):\n",
    "    \"\"\"Build a batched tf.data.Dataset from (path, score) samples.\n",
    "\n",
    "    Notes:\n",
    "      - When training=True the dataset is shuffled deterministically (seeded by SEED)\n",
    "      - Uses AUTOTUNE for parallel preprocessing\n",
    "    \"\"\"\n",
    "    paths, scores = zip(*samples)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((list(paths), list(scores)))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=len(paths), seed=SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model",
   "metadata": {},
   "source": [
    "## 5) Define a lightweight regression model\n",
    "- BiLSTM + pooling over landmark sequences; single sigmoid output predicts normalized score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "build-model",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">133</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">268,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m133\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m268,288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">309,505</span> (1.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m309,505\u001b[0m (1.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">309,505</span> (1.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m309,505\u001b[0m (1.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FEATURE_DIMS = NUM_LANDMARKS * LANDMARK_DIMS + 1\n",
    "\n",
    "def build_model() -> tf.keras.Model:\n",
    "    inputs = tf.keras.Input(shape=(NUM_FRAMES, FEATURE_DIMS))\n",
    "    # no global masking — we append a per-frame \"valid\" flag as one feature\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(inputs)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)  # normalized score 0-1\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"mse\",\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train",
   "metadata": {},
   "source": [
    "## 6) Train\n",
    "- Early stopping on validation MAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "train-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 0.1843 - mae: 0.4100 - val_loss: 0.1469 - val_mae: 0.3580\n",
      "Epoch 2/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 0.1843 - mae: 0.4100 - val_loss: 0.1469 - val_mae: 0.3580\n",
      "Epoch 2/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.1426 - mae: 0.3501 - val_loss: 0.1274 - val_mae: 0.3198\n",
      "Epoch 3/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.1426 - mae: 0.3501 - val_loss: 0.1274 - val_mae: 0.3198\n",
      "Epoch 3/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1162 - mae: 0.3027 - val_loss: 0.1194 - val_mae: 0.2895\n",
      "Epoch 4/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1162 - mae: 0.3027 - val_loss: 0.1194 - val_mae: 0.2895\n",
      "Epoch 4/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1104 - mae: 0.2796 - val_loss: 0.1146 - val_mae: 0.2713\n",
      "Epoch 5/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1104 - mae: 0.2796 - val_loss: 0.1146 - val_mae: 0.2713\n",
      "Epoch 5/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1013 - mae: 0.2607 - val_loss: 0.1112 - val_mae: 0.2601\n",
      "Epoch 6/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1013 - mae: 0.2607 - val_loss: 0.1112 - val_mae: 0.2601\n",
      "Epoch 6/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1026 - mae: 0.2533 - val_loss: 0.1082 - val_mae: 0.2496\n",
      "Epoch 7/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1026 - mae: 0.2533 - val_loss: 0.1082 - val_mae: 0.2496\n",
      "Epoch 7/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0890 - mae: 0.2274 - val_loss: 0.1044 - val_mae: 0.2401\n",
      "Epoch 8/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0890 - mae: 0.2274 - val_loss: 0.1044 - val_mae: 0.2401\n",
      "Epoch 8/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0882 - mae: 0.2156 - val_loss: 0.0992 - val_mae: 0.2317\n",
      "Epoch 9/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0882 - mae: 0.2156 - val_loss: 0.0992 - val_mae: 0.2317\n",
      "Epoch 9/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0943 - mae: 0.2267 - val_loss: 0.0918 - val_mae: 0.2217\n",
      "Epoch 10/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0943 - mae: 0.2267 - val_loss: 0.0918 - val_mae: 0.2217\n",
      "Epoch 10/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0845 - mae: 0.2227 - val_loss: 0.0855 - val_mae: 0.2129\n",
      "Epoch 11/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0845 - mae: 0.2227 - val_loss: 0.0855 - val_mae: 0.2129\n",
      "Epoch 11/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0781 - mae: 0.2132 - val_loss: 0.0809 - val_mae: 0.2052\n",
      "Epoch 12/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0781 - mae: 0.2132 - val_loss: 0.0809 - val_mae: 0.2052\n",
      "Epoch 12/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0825 - mae: 0.2215 - val_loss: 0.0755 - val_mae: 0.1992\n",
      "Epoch 13/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0825 - mae: 0.2215 - val_loss: 0.0755 - val_mae: 0.1992\n",
      "Epoch 13/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0743 - mae: 0.2061 - val_loss: 0.0707 - val_mae: 0.1930\n",
      "Epoch 14/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0743 - mae: 0.2061 - val_loss: 0.0707 - val_mae: 0.1930\n",
      "Epoch 14/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0674 - mae: 0.2021 - val_loss: 0.0677 - val_mae: 0.1871\n",
      "Epoch 15/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0674 - mae: 0.2021 - val_loss: 0.0677 - val_mae: 0.1871\n",
      "Epoch 15/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0659 - mae: 0.1913 - val_loss: 0.0657 - val_mae: 0.1827\n",
      "Epoch 16/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0659 - mae: 0.1913 - val_loss: 0.0657 - val_mae: 0.1827\n",
      "Epoch 16/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0639 - mae: 0.1896 - val_loss: 0.0628 - val_mae: 0.1783\n",
      "Epoch 17/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0639 - mae: 0.1896 - val_loss: 0.0628 - val_mae: 0.1783\n",
      "Epoch 17/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0581 - mae: 0.1814 - val_loss: 0.0586 - val_mae: 0.1734\n",
      "Epoch 18/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0581 - mae: 0.1814 - val_loss: 0.0586 - val_mae: 0.1734\n",
      "Epoch 18/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0560 - mae: 0.1796 - val_loss: 0.0553 - val_mae: 0.1701\n",
      "Epoch 19/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0560 - mae: 0.1796 - val_loss: 0.0553 - val_mae: 0.1701\n",
      "Epoch 19/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0517 - mae: 0.1712 - val_loss: 0.0532 - val_mae: 0.1668\n",
      "Epoch 20/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0517 - mae: 0.1712 - val_loss: 0.0532 - val_mae: 0.1668\n",
      "Epoch 20/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0571 - mae: 0.1771 - val_loss: 0.0509 - val_mae: 0.1637\n",
      "Epoch 21/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0571 - mae: 0.1771 - val_loss: 0.0509 - val_mae: 0.1637\n",
      "Epoch 21/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0483 - mae: 0.1591 - val_loss: 0.0519 - val_mae: 0.1604\n",
      "Epoch 22/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0483 - mae: 0.1591 - val_loss: 0.0519 - val_mae: 0.1604\n",
      "Epoch 22/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0494 - mae: 0.1654 - val_loss: 0.0523 - val_mae: 0.1587\n",
      "Epoch 23/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0494 - mae: 0.1654 - val_loss: 0.0523 - val_mae: 0.1587\n",
      "Epoch 23/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0434 - mae: 0.1500 - val_loss: 0.0491 - val_mae: 0.1562\n",
      "Epoch 24/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0434 - mae: 0.1500 - val_loss: 0.0491 - val_mae: 0.1562\n",
      "Epoch 24/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0470 - mae: 0.1581 - val_loss: 0.0461 - val_mae: 0.1543\n",
      "Epoch 25/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0470 - mae: 0.1581 - val_loss: 0.0461 - val_mae: 0.1543\n",
      "Epoch 25/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0422 - mae: 0.1554 - val_loss: 0.0446 - val_mae: 0.1513\n",
      "Epoch 26/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0422 - mae: 0.1554 - val_loss: 0.0446 - val_mae: 0.1513\n",
      "Epoch 26/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0389 - mae: 0.1479 - val_loss: 0.0462 - val_mae: 0.1500\n",
      "Epoch 27/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0389 - mae: 0.1479 - val_loss: 0.0462 - val_mae: 0.1500\n",
      "Epoch 27/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0369 - mae: 0.1421 - val_loss: 0.0473 - val_mae: 0.1488\n",
      "Epoch 28/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0369 - mae: 0.1421 - val_loss: 0.0473 - val_mae: 0.1488\n",
      "Epoch 28/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0363 - mae: 0.1343 - val_loss: 0.0454 - val_mae: 0.1462\n",
      "Epoch 29/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0363 - mae: 0.1343 - val_loss: 0.0454 - val_mae: 0.1462\n",
      "Epoch 29/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0298 - mae: 0.1287 - val_loss: 0.0423 - val_mae: 0.1422\n",
      "Epoch 30/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0298 - mae: 0.1287 - val_loss: 0.0423 - val_mae: 0.1422\n",
      "Epoch 30/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - mae: 0.1293 - val_loss: 0.0421 - val_mae: 0.1418\n",
      "Epoch 31/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - mae: 0.1293 - val_loss: 0.0421 - val_mae: 0.1418\n",
      "Epoch 31/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0313 - mae: 0.1301 - val_loss: 0.0431 - val_mae: 0.1427\n",
      "Epoch 32/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0313 - mae: 0.1301 - val_loss: 0.0431 - val_mae: 0.1427\n",
      "Epoch 32/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0242 - mae: 0.1148 - val_loss: 0.0400 - val_mae: 0.1383\n",
      "Epoch 33/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0242 - mae: 0.1148 - val_loss: 0.0400 - val_mae: 0.1383\n",
      "Epoch 33/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0218 - mae: 0.1136 - val_loss: 0.0407 - val_mae: 0.1388\n",
      "Epoch 34/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0218 - mae: 0.1136 - val_loss: 0.0407 - val_mae: 0.1388\n",
      "Epoch 34/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0213 - mae: 0.1085 - val_loss: 0.0420 - val_mae: 0.1396\n",
      "Epoch 35/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0213 - mae: 0.1085 - val_loss: 0.0420 - val_mae: 0.1396\n",
      "Epoch 35/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - mae: 0.1180 - val_loss: 0.0390 - val_mae: 0.1350\n",
      "Epoch 36/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - mae: 0.1180 - val_loss: 0.0390 - val_mae: 0.1350\n",
      "Epoch 36/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0202 - mae: 0.1038 - val_loss: 0.0413 - val_mae: 0.1375\n",
      "Epoch 37/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0202 - mae: 0.1038 - val_loss: 0.0413 - val_mae: 0.1375\n",
      "Epoch 37/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0212 - mae: 0.1069 - val_loss: 0.0435 - val_mae: 0.1396\n",
      "Epoch 38/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0212 - mae: 0.1069 - val_loss: 0.0435 - val_mae: 0.1396\n",
      "Epoch 38/400\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0193 - mae: 0.0987 - val_loss: 0.0460 - val_mae: 0.1417\n",
      "Best val MAE (normalized 0-1): 0.13504156470298767\n",
      "Best val MAE (score units): 13.504156470298767\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0193 - mae: 0.0987 - val_loss: 0.0460 - val_mae: 0.1417\n",
      "Best val MAE (normalized 0-1): 0.13504156470298767\n",
      "Best val MAE (score units): 13.504156470298767\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor=\"val_mae\"),\n",
    "    tf.keras.callbacks.ModelCheckpoint(str(MODEL_DIR / \"model.keras\"), save_best_only=True, monitor=\"val_mae\"),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=400,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "best_val_mae = min(history.history[\"val_mae\"])\n",
    "print(\"Best val MAE (normalized 0-1):\", best_val_mae)\n",
    "print(\"Best val MAE (score units):\", best_val_mae * SCORE_SCALE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate",
   "metadata": {},
   "source": [
    "## 7) Evaluate and save artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eval-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 748ms/step - loss: 0.0457 - mae: 0.1627\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 748ms/step - loss: 0.0457 - mae: 0.1627\n",
      "{'loss': 0.045712172985076904, 'mae': 0.16267424821853638}\n",
      "MAE in score units: 16.27\n",
      "Saving single-file Keras archive -> checkpoints\\squat_scorer.keras\n",
      "Exporting SavedModel (model.export) -> checkpoints\\squat_scorer_savedmodel\n",
      "{'loss': 0.045712172985076904, 'mae': 0.16267424821853638}\n",
      "MAE in score units: 16.27\n",
      "Saving single-file Keras archive -> checkpoints\\squat_scorer.keras\n",
      "Exporting SavedModel (model.export) -> checkpoints\\squat_scorer_savedmodel\n",
      "INFO:tensorflow:Assets written to: checkpoints\\squat_scorer_savedmodel\\assets\n",
      "INFO:tensorflow:Assets written to: checkpoints\\squat_scorer_savedmodel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints\\squat_scorer_savedmodel\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'checkpoints\\squat_scorer_savedmodel'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 16, 133), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1430656486384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656487088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656485680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656486208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656486912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656486736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656487792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656485856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656487264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656489024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656488496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1430656489728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Artifacts saved to checkpoints\n",
      "Artifacts saved to checkpoints\n"
     ]
    }
   ],
   "source": [
    "eval_target = test_ds if test_ds is not None else val_ds\n",
    "eval_results = model.evaluate(eval_target, return_dict=True)\n",
    "print(eval_results)\n",
    "print(f\"MAE in score units: {eval_results['mae'] * SCORE_SCALE:.2f}\")\n",
    "\n",
    "# Save both a single-file Keras archive and a SavedModel directory for robust conversion.\n",
    "keras_export_file = MODEL_DIR / \"squat_scorer.keras\"\n",
    "saved_model_dir = MODEL_DIR / \"squat_scorer_savedmodel\"\n",
    "\n",
    "print('Saving single-file Keras archive ->', keras_export_file)\n",
    "model.save(str(keras_export_file))\n",
    "\n",
    "# Keras 3: prefer model.export(...) to write a SavedModel dir; fall back to tf.saved_model.save\n",
    "try:\n",
    "    print('Exporting SavedModel (model.export) ->', saved_model_dir)\n",
    "    model.export(str(saved_model_dir))\n",
    "except Exception:\n",
    "    try:\n",
    "        print('Fallback export via tf.saved_model.save ->', saved_model_dir)\n",
    "        tf.saved_model.save(model, str(saved_model_dir))\n",
    "    except Exception as e:\n",
    "        print('SavedModel export failed:', e)\n",
    "\n",
    "with (MODEL_DIR / \"score_scale.txt\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(str(SCORE_SCALE))\n",
    "\n",
    "print(\"Artifacts saved to\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c66451",
   "metadata": {},
   "source": [
    "## 8) Export TFLite for Android\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fbbe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote checkpoints\\squat_scorer.tflite\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path(\"checkpoints\")\n",
    "saved_model_dir = MODEL_DIR / \"squat_scorer_savedmodel\"  # or wherever you exported\n",
    "tflite_path = MODEL_DIR / \"squat_scorer.tflite\"\n",
    "\n",
    "# Create converter from SavedModel -> TFLite. SavedModel is preferred for complex models\n",
    "# because it preserves the computation graph and provides a robust conversion path.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_dir))\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Key flags for TensorList + LSTM\n",
    "# - experimental_enable_resource_variables: helps the converter handle resource variables\n",
    "# - _experimental_lower_tensor_list_ops: controls lowering of TensorList ops\n",
    "# - target_spec.supported_ops: adding SELECT_TF_OPS allows Flex ops to be included for\n",
    "#   TF-only ops (e.g., while/tensorlist constructs used by LSTM). Note that SELECT_TF_OPS\n",
    "#   requires the TensorFlow flex runtime on the target (increases APK/binary size).\n",
    "converter.experimental_enable_resource_variables = True\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS,\n",
    "]\n",
    "\n",
    "# Convert and write.\n",
    "tflite_model = converter.convert()\n",
    "tflite_path.write_bytes(tflite_model)\n",
    "print(\"Wrote\", tflite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference",
   "metadata": {},
   "source": [
    "## 9) Single-sample inference helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infer-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(video_path: str):\n",
    "    \"\"\"Single-sample prediction helper for interactive testing.\n",
    "\n",
    "    - Accepts: path to a video file on disk\n",
    "    - Returns: predicted score in human units (0..SCORE_SCALE)\n",
    "\n",
    "    The function uses the same preprocessing pipeline to extract the NUM_FRAMES keypoint\n",
    "    sequence and reshapes it to the model input shape: (1, NUM_FRAMES, FEATURE_DIMS).\n",
    "    \"\"\"\n",
    "    keypoints = _extract_keypoints_np(video_path)\n",
    "    # keypoints shape: (NUM_FRAMES, FEATURE_DIMS) where FEATURE_DIMS = NUM_LANDMARKS*LANDMARK_DIMS + 1\n",
    "    keypoints = keypoints.reshape(1, NUM_FRAMES, NUM_LANDMARKS * LANDMARK_DIMS + 1)\n",
    "\n",
    "    # model.predict returns a normalized scalar in 0..1; scale up to the human-readable range\n",
    "    score_norm = float(model.predict(keypoints, verbose=0)[0][0])\n",
    "    return score_norm * SCORE_SCALE\n",
    "\n",
    "\n",
    "# Example usage (interactive): pick a path from train_samples if available\n",
    "example_path = train_samples[0][0] if train_samples else labeled_samples[0][0]\n",
    "pred_score = predict_sample(example_path)\n",
    "print(f\"Predicted score: {pred_score:.2f} (0-{int(SCORE_SCALE)}) on {example_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
